# COVID19_radiography_detection

## **About the Project**
This project attempts to detect through the use of data and image analysis tools in the field of Machine Learning, such as convolutional neural networks or deep neural networks, the detection of COVID-19 SARS COV2 in chest X-ray images.

Through the use of neural networks in the field of Machine Learning, it is expected to have an accuracy greater than 85% in the detection of COVID-19 SARS COV2.

The intention of this project is directed to the medical community to provide additional diagnostics in the detection of COVID-19 SARS COV2.


<img src="./media/coronovirus.jpg" />

---
## **Data**

We are going to collect data from Kaggle datasets in order to have radiography for people with COVID-19 positive cases along with Normal or healthy people images.

---

## **Requirements**
||||||
:---:|:---:|:---:|:---:|:---:
<a target="_blank" href="https://anaconda.org/"><img src="./media/anaconda_logo.jpg" alt="anaconda" width=115/></a>|<a target="_blank" href="https://www.tensorflow.org/"><img src="./media/tensorflow_logo.jpg" alt="tensorflow" /></a>|<a target="_blank" href="https://pandas.pydata.org/"><img src="./media/pandas_logo.jpg" alt="pandas" /></a>|<a target="_blank" href="http://www.numpy.org/"><img src="./media/numpy_logo.jpg" alt="numpy" /></a>|<a target="_blank" href="https://keras.io/"><img src="./media/keras_logo.jpg" alt="keras" /></a>|
<a target="_blank" href="https://www.python.org/"><img src="./media/python_logo.jpg" alt="python" width=60/></a>|<a target="_blank" href="https://scikit-learn.org/stable/"><img src="./media/sklearn_logo.jpg" alt="sklearn" width=200/></a>|<a target="_blank" href="https://matplotlib.org/stable/index.html"><img src="./media/matplotlib_logo.jpg" alt="matplotlib" width=200/></a>|<a target="_blank" href="https://www.w3schools.com/python/numpy/numpy_random_seaborn.asp"><img src="./media/seaborn_logo.jpg" alt="seaborn" width=150/></a>|<a target="_blank" href="https://pypi.org/project/opencv-python/"><img src="./media/cv2_logo.jpg" alt="cv2" width=150/></a>

---

## **How many images are required to train a CNN?**
5.000 images may be too few for a Convolutional Neural Network(CNN), in fact, research projects carried out by NVIDIA have trained around 100,000 diagnostic images of OCT (optical coherence tomography), with excellent results.

Currently our model has around 7,200 images that were generated by x-ray radiography.

So as the target data set is large and different from the base data set, we created a training model, which will fit our needs, you can see this model in this [link](model.py).

---

## **Setup Environment**
> To get started on Ubuntu 20.04...
### Step 1
- Install <a href="https://www.anaconda.com/distribution/
">Anaconda </a><img src="./media/anaconda_logo.jpg" width=30 />
- Open your terminal a type
    ~~~ 
    cd /tmp
    curl https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh --output anaconda.sh
    ~~~
- Verify the integrity of the installer's data
    ~~~ 
    sha256sum anaconda.sh
    ~~~
    ***Output***
    ~~~
    2b9f088b2022edb474915d9f69a803d6449d5fdb4c303041f60ac4aefcc208bb  anaconda.sh
    ~~~
- Run Anaconda script
    ~~~
    bash anaconda.sh
    ~~~
    > You will receive the following result to review the license agreement by pressing ENTER until you reach the end.

    ***Output***
    ~~~
    Welcome to Anaconda3 2020.02

    In order to continue the installation process, please review the license
    agreement.
    Please, press ENTER to continue
    >>>  
    ~~~
    _When you reach the end of the license, type yes, if you accept the license, to complete the installation._
    >When the installation is complete, you will receive the following output:
    ~~~
    ...
    installation finished.
    Do you wish the installer to initialize Anaconda3
    by running conda init? [yes|no]
    [no] >>>  
    ~~~
- Activate the installation
    ~~~
    source ~/.bashrc
    ~~~
- Set up Anaconda environments
    >It is good practice to create new environments for each of your projects. To create a Python 3 environment called my_env, the syntax is as follows:
    ~~~
    conda create --name my_env python=3
    ~~~
    >You can activate your new environment by typing the following:
    ~~~
    conda activate my_env
    ~~~
    > When you're ready to disable your Anaconda environment, you can do so by typing the following:
    ~~~
    conda deactivate
    ~~~
### Step 2
- Prepare our anaconda environment by typing the following on the terminal (make sure to install the libraries listed below):
    ~~~
    conda activate my_env
    sudo apt-get -y install python3-pip
    """Libraries to install"""
    from __future__ import absolute_import, division, print_function, unicode_literals
    import pandas as pd
    import tensorflow as tf
    from tensorflow.keras import datasets, layers, models
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    from tqdm import tqdm
    import cv2
    from glob import glob
    import sklearn
    import skimage
    from skimage.transform import resize
    import random
    import datetime
    from imblearn.over_sampling import RandomOverSampler
    from imblearn.under_sampling import RandomUnderSampler
    from skimage.color import rgb2gray
    import seaborn as sns
    git clone https://github.com/oscarmrt/COVID19_radiography_detection.git
    cd COVID19_radiography_detection
    mkdir chest_xray
    cd chest_xray
    ~~~

- In the data folder you have to copy the follow images:
[drive images](https://drive.google.com/drive/folders/1jgE1mWYO43CSeArlU9zUlXm994CP9A9Q?usp=sharing) this is our x-ray images we used to train the model

### Step 3
- In this moment you have to be in the data folder so lets go to move to the principal path typing **`cd ..`** after that we have to write the following:
    ~~~
    ./model.py
    ~~~

- If everything is ok you can get a result like this in your terminal

    <img src="./media/result1.jpg" />

---

## **Results**
~~~
Epoch 1/35
7/7 [==============================] - 5s 719ms/step - loss: 0.9381 - accuracy: 0.4779 - val_loss: 0.6935 - val_accuracy: 0.5000
Epoch 2/35
7/7 [==============================] - 4s 569ms/step - loss: 0.6936 - accuracy: 0.5441 - val_loss: 0.6924 - val_accuracy: 0.6000
Epoch 3/35
7/7 [==============================] - 4s 592ms/step - loss: 0.6892 - accuracy: 0.6187 - val_loss: 0.6887 - val_accuracy: 0.6500
Epoch 4/35
7/7 [==============================] - 4s 604ms/step - loss: 0.6781 - accuracy: 0.7168 - val_loss: 0.6988 - val_accuracy: 0.5000
Epoch 5/35
7/7 [==============================] - 4s 544ms/step - loss: 0.6482 - accuracy: 0.5975 - val_loss: 0.6871 - val_accuracy: 0.5500
Epoch 6/35
7/7 [==============================] - 4s 565ms/step - loss: 0.5812 - accuracy: 0.7397 - val_loss: 0.7176 - val_accuracy: 0.5500
Epoch 7/35
7/7 [==============================] - 4s 569ms/step - loss: 0.6259 - accuracy: 0.6630 - val_loss: 0.6835 - val_accuracy: 0.5833
Epoch 8/35
7/7 [==============================] - 4s 581ms/step - loss: 0.5448 - accuracy: 0.7342 - val_loss: 0.6443 - val_accuracy: 0.6500
Epoch 9/35
7/7 [==============================] - 4s 566ms/step - loss: 0.5119 - accuracy: 0.7898 - val_loss: 0.6214 - val_accuracy: 0.6167
Epoch 10/35
7/7 [==============================] - 4s 594ms/step - loss: 0.5190 - accuracy: 0.7445 - val_loss: 0.5973 - val_accuracy: 0.5833
Epoch 11/35
7/7 [==============================] - 4s 542ms/step - loss: 0.5396 - accuracy: 0.7155 - val_loss: 0.6151 - val_accuracy: 0.7000
Epoch 12/35
7/7 [==============================] - 4s 554ms/step - loss: 0.4559 - accuracy: 0.7962 - val_loss: 0.5664 - val_accuracy: 0.7000
Epoch 13/35
7/7 [==============================] - 4s 555ms/step - loss: 0.4133 - accuracy: 0.8038 - val_loss: 0.6769 - val_accuracy: 0.7000
Epoch 14/35
7/7 [==============================] - 4s 575ms/step - loss: 0.4183 - accuracy: 0.8215 - val_loss: 0.5820 - val_accuracy: 0.7333
Epoch 15/35
7/7 [==============================] - 4s 593ms/step - loss: 0.3948 - accuracy: 0.8177 - val_loss: 0.5052 - val_accuracy: 0.7167
Epoch 16/35
7/7 [==============================] - 4s 576ms/step - loss: 0.3229 - accuracy: 0.8661 - val_loss: 0.4191 - val_accuracy: 0.7333
Epoch 17/35
7/7 [==============================] - 4s 588ms/step - loss: 0.3235 - accuracy: 0.8501 - val_loss: 0.4015 - val_accuracy: 0.7667
Epoch 18/35
7/7 [==============================] - 4s 619ms/step - loss: 0.3115 - accuracy: 0.8913 - val_loss: 0.5566 - val_accuracy: 0.6833
Epoch 19/35
7/7 [==============================] - 4s 650ms/step - loss: 0.3257 - accuracy: 0.8512 - val_loss: 0.3918 - val_accuracy: 0.8167
Epoch 20/35
7/7 [==============================] - 4s 584ms/step - loss: 0.3460 - accuracy: 0.8119 - val_loss: 0.3435 - val_accuracy: 0.8667
Epoch 21/35
7/7 [==============================] - 4s 606ms/step - loss: 0.2737 - accuracy: 0.8790 - val_loss: 0.3651 - val_accuracy: 0.8333
Epoch 22/35
7/7 [==============================] - 4s 562ms/step - loss: 0.2587 - accuracy: 0.8741 - val_loss: 0.2720 - val_accuracy: 0.9000
Epoch 23/35
7/7 [==============================] - 4s 607ms/step - loss: 0.2355 - accuracy: 0.8798 - val_loss: 0.4026 - val_accuracy: 0.8333
Epoch 24/35
7/7 [==============================] - 4s 606ms/step - loss: 0.2178 - accuracy: 0.9103 - val_loss: 0.2065 - val_accuracy: 0.9333
Epoch 25/35
7/7 [==============================] - 4s 564ms/step - loss: 0.1550 - accuracy: 0.9356 - val_loss: 0.3326 - val_accuracy: 0.8333
Epoch 26/35
7/7 [==============================] - 4s 583ms/step - loss: 0.1722 - accuracy: 0.9416 - val_loss: 0.3908 - val_accuracy: 0.8167
Epoch 27/35
7/7 [==============================] - 4s 605ms/step - loss: 0.1935 - accuracy: 0.9252 - val_loss: 0.2397 - val_accuracy: 0.9167
Epoch 28/35
7/7 [==============================] - 4s 616ms/step - loss: 0.1489 - accuracy: 0.9692 - val_loss: 0.1458 - val_accuracy: 0.9500
Epoch 29/35
7/7 [==============================] - 4s 612ms/step - loss: 0.1395 - accuracy: 0.9405 - val_loss: 0.7241 - val_accuracy: 0.7000
Epoch 30/35
7/7 [==============================] - 4s 604ms/step - loss: 0.4520 - accuracy: 0.8178 - val_loss: 0.2306 - val_accuracy: 0.9167
Epoch 31/35
7/7 [==============================] - 4s 568ms/step - loss: 0.2291 - accuracy: 0.9099 - val_loss: 0.2700 - val_accuracy: 0.9000
Epoch 32/35
7/7 [==============================] - 4s 557ms/step - loss: 0.1771 - accuracy: 0.9559 - val_loss: 0.1595 - val_accuracy: 0.9667
Epoch 33/35
7/7 [==============================] - 4s 566ms/step - loss: 0.1002 - accuracy: 0.9788 - val_loss: 0.1153 - val_accuracy: 0.9500
Epoch 34/35
7/7 [==============================] - 4s 578ms/step - loss: 0.1221 - accuracy: 0.9451 - val_loss: 0.1058 - val_accuracy: 0.9667
Epoch 35/35
7/7 [==============================] - 4s 567ms/step - loss: 0.1011 - accuracy: 0.9804 - val_loss: 0.1458 - val_accuracy: 0.9500
~~~

As we can see, we had a very good result, higher than the expected 85%.

---

## **Conclusions**
We can conclude that the current model has an accuracy of more than 90%, this means that the proposed model satisfactorily meets the general objective of the project, exceeding 90% accuracy in detecting COVID19 due to pneumonia, during each of the periods. , it was determined that to successfully conclude the project it must be carried out with the mainstreaming of the validation method, this is achieved by dividing the data into two parts: the training set and the validation set, the latter allowed us to validate the performance of the trainees model with training data. Finally, it is clarified that if there is a large difference between the precision by the training model and the validation of the precision, it is declared as an adjusted model, instead if the validation precision (val_acc) is equal to or a little less than precision training (acc) is declared as a good model.

---

## **acknowledgment**
- On behalf of the team, we want to thank Holberton sincerely for the Machine Learning advanced program, for teaching us about soft skills and learning to learn. ![follow me](https://img.shields.io/twitter/follow/HolbertonCOL?style=social) ![follow me](https://img.shields.io/twitter/follow/holbertonschool?style=social)

## **Authors**
| | | | | |
:---:|:---:|:---:|:---:|:---:
**Samir Millan**<a><img src="./media/samir.jpg" width=145 />[![GitHub followers](https://img.shields.io/github/followers/gaspela?label=Follow%20me&style=social)](https://github.com/gaspela)</a><a href="http://twitter.com/@gaspela" target="_blank">![follow me](https://img.shields.io/twitter/follow/gaspela?style=social)</a>|**Oscar Rodriguez**<a><img src="./media/oscar.jpg" width=145/>[![GitHub followers](https://img.shields.io/github/followers/oscarmrt?label=Follow%20me&style=social)](https://github.com/oscarmrt)</a><a href="http://twitter.com/@OscaRT07" target="_blank">![follow me](https://img.shields.io/twitter/follow/OscaRT07?style=social)</a>|**Diego Quijano**<a><img src="./media/diego.jpg" width=145/></a>[![GitHub followers](https://img.shields.io/github/followers/diego0096?label=Follow%20me&style=social)](https://github.com/diego0096)<a href="http://twitter.com/@DiegoQ0096" target="_blank">![follow me](https://img.shields.io/twitter/follow/DiegoQ0096?style=social)</a>
